diff --git build.xml build.xml
index 1edc448..6be8678 100644
--- build.xml
+++ build.xml
@@ -237,6 +237,7 @@
         <fileset dir="${ivy.lib.dir}" includes="hadoop*.jar"/>
         <path refid="classpath"/>
         <path refid="test-classpath"/>
+        <fileset file="${lib.dir}/sdsuLibJKD12.jar"/>
     </path>
 
     <target name="init" depends="ivy-compile" >
@@ -701,6 +702,20 @@
     </target>
 
     <!-- ================================================================== -->
+    <!-- Make pigperf.jar                                                   -->
+    <!-- ================================================================== -->
+    <target name="pigperf" depends="compile-test" description="Create pigperf.jar">
+        <jar jarfile="pigperf.jar">
+            <fileset dir="${test.build.dir}/classes">
+                <include name="org/apache/pig/test/pigmix/**"/>
+                <include name="org/apache/pig/test/utils/datagen/*"/>
+                <include name="org/apache/pig/test/udf/storefunc/*"/>
+            </fileset>
+            <zipfileset src="${lib.dir}/sdsuLibJKD12.jar" />
+        </jar>
+    </target>
+
+    <!-- ================================================================== -->
     <!-- Run unit tests                                                     -->
     <!-- ================================================================== -->
     <target name="test-core" depends="compile-test,jar-withouthadoop" description="Run full set of unit tests">
diff --git test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java
index 6d30bc6..c9eda03 100644
--- test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java
+++ test/e2e/pig/udfs/java/org/apache/pig/test/udf/storefunc/PigPerformanceLoader.java
@@ -54,13 +54,8 @@ public class PigPerformanceLoader extends PigStorage {
     class Caster implements LoadCaster {
         
         Utf8StorageConverter helper = new Utf8StorageConverter();
-        /**
-         * 
-         */
-        public Caster() {
-            // TODO Auto-generated constructor stub
-        }
         
+        @Override
         public DataBag bytesToBag(byte[] b, ResourceFieldSchema fs) throws IOException {
             if (b == null) return null;
 
@@ -100,10 +95,12 @@ public class PigPerformanceLoader extends PigStorage {
             return bag;
         }
 
+		@Override
 		public Map<String, Object> bytesToMap(byte[] b, ResourceFieldSchema fieldSchema) throws IOException {
-			throw new UnsupportedOperationException();
+			return helper.bytesToMap(b);
 		}
 
+        @Override
         public Map<String, Object> bytesToMap(byte[] b) throws IOException {
             if (b == null) return null;
 
diff --git test/org/apache/pig/test/pigmix/mapreduce/L1.java test/org/apache/pig/test/pigmix/mapreduce/L1.java
new file mode 100644
index 0000000..c0dd371
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L1.java
@@ -0,0 +1,156 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L1 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, IntWritable> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, IntWritable> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            int cnt = 0;
+            if (fields.get(1).toString() == "1") {
+                Text throwAway = Library.mapLookup(fields.get(7), new Text("a"));
+                cnt++;
+            } else {
+                List<Text> le = Library.splitLine(fields.get(8), '');
+                for (Text e : le) {
+                    Text throwAway = Library.mapLookup(e, new Text("b"));
+                    cnt++;
+                }
+            }
+            oc.collect(fields.get(0), new IntWritable(cnt));
+        }
+    }
+
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, IntWritable, Text, IntWritable> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<IntWritable> iter,
+                OutputCollector<Text, IntWritable> oc,
+                Reporter reporter) throws IOException {
+            int cnt = 0;
+            while (iter.hasNext()) {
+                cnt += iter.next().get();
+            }
+            oc.collect(key, new IntWritable(cnt));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L1.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(IntWritable.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(Group.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L1out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L1 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L10.java test/org/apache/pig/test/pigmix/mapreduce/L10.java
new file mode 100644
index 0000000..fa23e49
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L10.java
@@ -0,0 +1,284 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.WritableComparable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Partitioner;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L10 {
+
+    public static class MyType implements WritableComparable<MyType> {
+
+        public String query_term;
+        int timespent;
+        double estimated_revenue;
+
+        public MyType() {
+            query_term = null;
+            timespent = 0;
+            estimated_revenue = 0.0;
+        }
+
+        public MyType(Text qt, Text ts, Text er) {
+            query_term = qt.toString();
+            try {
+                timespent = Integer.valueOf(ts.toString());
+            } catch (NumberFormatException nfe) {
+                timespent = 0;
+            }
+            try {
+                estimated_revenue = Double.valueOf(er.toString());
+            } catch (NumberFormatException nfe) {
+                estimated_revenue = 0.0;
+            }
+        }
+
+        @Override
+        public void write(DataOutput out) throws IOException {
+            out.writeInt(timespent);
+            out.writeDouble(estimated_revenue);
+            out.writeInt(query_term.length());
+            out.writeBytes(query_term);
+        }
+
+        @Override
+        public void readFields(DataInput in) throws IOException {
+            timespent = in.readInt();
+            estimated_revenue = in.readDouble();
+            int len = in.readInt();
+            byte[] b = new byte[len];
+            in.readFully(b);
+            query_term = new String(b);
+        }
+
+        @Override
+        public int compareTo(MyType other) {
+            int rc = query_term.compareTo(other.query_term);
+            if (rc != 0) return rc;
+            if (estimated_revenue < other.estimated_revenue) return 1;
+            else if (estimated_revenue > other.estimated_revenue) return -1;
+            if (timespent < other.timespent) return -1;
+            else if (timespent > other.timespent) return 1;
+            return 0;
+        }
+    }
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, MyType, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<MyType, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            oc.collect(new MyType(fields.get(3), fields.get(2), fields.get(6)),
+                val);
+        }
+    }
+
+    public static class MyPartitioner implements Partitioner<MyType, Text> {
+
+        public Map<Character, Integer> map;
+
+        @Override
+        public int getPartition(MyType key, Text value, int numPartitions) {
+            int rc = 0;
+            if (key.query_term == null || key.query_term.length() < 2) return 39;
+            if (key.query_term.charAt(0) > ']') rc += 20;
+            rc += map.get(key.query_term.charAt(1));
+            return rc;
+        }
+
+        @Override
+        public void configure(JobConf conf) {
+            // Don't actually do any configuration, do the setup of the hash
+            // because this call is guaranteed to be made each time we set up
+            // MyPartitioner
+            map = new HashMap<Character, Integer>(57);
+            map.put('A', 0);
+            map.put('B', 1);
+            map.put('C', 2);
+            map.put('D', 3);
+            map.put('E', 4);
+            map.put('F', 5);
+            map.put('G', 6);
+            map.put('I', 7);
+            map.put('H', 8);
+            map.put('J', 9);
+            map.put('K', 10);
+            map.put('L', 11);
+            map.put('M', 12);
+            map.put('N', 13);
+            map.put('O', 14);
+            map.put('P', 15);
+            map.put('Q', 16);
+            map.put('R', 17);
+            map.put('S', 18);
+            map.put('T', 19);
+            map.put('U', 0);
+            map.put('V', 1);
+            map.put('W', 2);
+            map.put('X', 3);
+            map.put('Y', 4);
+            map.put('Z', 5);
+            map.put('[', 6);
+            map.put('\\', 7);
+            map.put(']', 8);
+            map.put('^', 9);
+            map.put('_', 10);
+            map.put('`', 11);
+            map.put('a', 12);
+            map.put('b', 13);
+            map.put('c', 14);
+            map.put('d', 15);
+            map.put('e', 16);
+            map.put('f', 17);
+            map.put('g', 18);
+            map.put('h', 19);
+            map.put('i', 0);
+            map.put('j', 1);
+            map.put('k', 2);
+            map.put('l', 3);
+            map.put('m', 4);
+            map.put('n', 5);
+            map.put('o', 6);
+            map.put('p', 7);
+            map.put('q', 8);
+            map.put('r', 9);
+            map.put('s', 10);
+            map.put('t', 11);
+            map.put('u', 12);
+            map.put('v', 13);
+            map.put('w', 14);
+            map.put('x', 15);
+            map.put('y', 16);
+            map.put('z', 17);
+        }
+    }
+
+    public static class Group extends MapReduceBase
+        implements Reducer<MyType, Text, MyType, Text> {
+
+        @Override
+        public void reduce(
+                MyType key,
+                Iterator<Text> iter,
+                OutputCollector<MyType, Text> oc,
+                Reporter reporter) throws IOException {
+            while (iter.hasNext()) {
+                oc.collect(null, iter.next());
+            }
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L10.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(MyType.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setReducerClass(Group.class);
+        lp.setPartitionerClass(MyPartitioner.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L10out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L10 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L11.java test/org/apache/pig/test/pigmix/mapreduce/L11.java
new file mode 100644
index 0000000..74806bb
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L11.java
@@ -0,0 +1,217 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.KeyValueTextInputFormat;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapred.lib.IdentityMapper;
+
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L11 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text>,
+        Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            List<Text> fields = Library.splitLine(val, '');
+            oc.collect(fields.get(0), new Text());
+        }
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Just take the key and the first value.
+            oc.collect(key, iter.next());
+        }
+    }
+
+    public static class ReadWideRow extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text>,
+        Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            List<Text> fields = Library.splitLine(val, '');
+            oc.collect(fields.get(0), new Text());
+        }
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            // Just take the key and the first value.
+            oc.collect(key, iter.next());
+        }
+    }
+
+    public static class Union extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            // Just take the key and the first value.
+            oc.collect(key, iter.next());
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        String user = System.getProperty("user.name");
+        JobConf lp = new JobConf(L11.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(ReadPageViews.class);
+        lp.setReducerClass(ReadPageViews.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+user+"/tmp/p"));
+        lp.setNumReduceTasks(40);
+        Job loadPages = new Job(lp);
+
+        JobConf lu = new JobConf(L11.class);
+        lu.setJobName("Load Widerow");
+        lu.setInputFormat(TextInputFormat.class);
+        lu.setOutputKeyClass(Text.class);
+        lu.setOutputValueClass(Text.class);
+        lu.setMapperClass(ReadWideRow.class);
+        lu.setCombinerClass(ReadWideRow.class);
+        lu.setReducerClass(ReadWideRow.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lu.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lu, new Path(dataDir, "widerow"));
+        FileOutputFormat.setOutputPath(lu, new Path("/user/"+user+"/tmp/wr"));
+        lu.setNumReduceTasks(40);
+        Job loadWideRow = new Job(lu);
+
+        JobConf join = new JobConf(L11.class);
+        join.setJobName("Union WideRow and Pages");
+        join.setInputFormat(KeyValueTextInputFormat.class);
+        join.setOutputKeyClass(Text.class);
+        join.setOutputValueClass(Text.class);
+        join.setMapperClass(IdentityMapper.class);
+        join.setCombinerClass(Union.class);
+        join.setReducerClass(Union.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            join.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/p"));
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/wr"));
+        FileOutputFormat.setOutputPath(join, new Path("/user/"+user+"/L11out"));
+        join.setNumReduceTasks(40);
+        Job joinJob = new Job(join);
+        joinJob.addDependingJob(loadPages);
+        joinJob.addDependingJob(loadWideRow);
+
+        JobControl jc = new JobControl("L11 join");
+        jc.addJob(loadPages);
+        jc.addJob(loadWideRow);
+        jc.addJob(joinJob);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L12.java test/org/apache/pig/test/pigmix/mapreduce/L12.java
new file mode 100644
index 0000000..6acf8fc
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L12.java
@@ -0,0 +1,259 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.DoubleWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L12 {
+
+    public static class HighestValuePagePerUser extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, DoubleWritable>,
+        Reducer<Text, DoubleWritable, Text, DoubleWritable> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, DoubleWritable> oc,
+                Reporter reporter) throws IOException {
+
+            List<Text> fields = Library.splitLine(val, '');
+
+            // Filter out null users and query terms.
+            if (fields.get(0).getLength() == 0 &&
+                    fields.get(3).getLength() == 0) return;
+            try {
+                oc.collect(fields.get(0),
+                    new DoubleWritable(Double.valueOf(fields.get(6).toString())));
+            } catch (NumberFormatException nfe) {
+                oc.collect(fields.get(0), new DoubleWritable(0));
+            }
+        }
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<DoubleWritable> iter,
+                OutputCollector<Text, DoubleWritable> oc,
+                Reporter reporter) throws IOException {
+            double max = Double.MIN_VALUE;
+
+            while (iter.hasNext()) {
+                double d = iter.next().get();
+                max = max > d ? max : d;
+            }
+
+            oc.collect(key, new DoubleWritable(max));
+        }
+    }
+
+    public static class TotalTimespentPerTerm extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, LongWritable>,
+        Reducer<Text, LongWritable, Text, LongWritable> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, LongWritable> oc,
+                Reporter reporter) throws IOException {
+            List<Text> fields = Library.splitLine(val, '');
+
+            // Filter out non-null users
+            if (fields.get(0).getLength() != 0) return;
+            try {
+                oc.collect(fields.get(3),
+                    new LongWritable(Long.valueOf(fields.get(2).toString())));
+            } catch (NumberFormatException nfe) {
+                oc.collect(fields.get(3), new LongWritable(0));
+            }
+
+        }
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<LongWritable> iter,
+                OutputCollector<Text, LongWritable> oc,
+                Reporter reporter) throws IOException {
+            long sum = 0;
+
+            while (iter.hasNext()) sum += iter.next().get();
+            oc.collect(key, new LongWritable(sum));
+        }
+    }
+
+    public static class QueriesPerAction extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, LongWritable>,
+        Reducer<Text, LongWritable, Text, LongWritable> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, LongWritable> oc,
+                Reporter reporter) throws IOException {
+            List<Text> fields = Library.splitLine(val, '');
+
+            // Filter out non-null users and non-null queries
+            if (fields.get(0).getLength() != 0 ||
+                    fields.get(3).getLength() != 0) return;
+            oc.collect(fields.get(1), new LongWritable(1));
+
+        }
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<LongWritable> iter,
+                OutputCollector<Text, LongWritable> oc,
+                Reporter reporter) throws IOException {
+            long cnt = 0;
+
+            while (iter.hasNext()) {
+                iter.next();
+                cnt++;
+            }
+            oc.collect(key, new LongWritable(cnt));
+        }
+    }
+
+
+    public static void main(String[] args) throws IOException {
+
+        String user = System.getProperty("user.name");
+        JobConf lp = new JobConf(L12.class);
+        lp.setJobName("Find Highest Value Page Per User");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(DoubleWritable.class);
+        lp.setMapperClass(HighestValuePagePerUser.class);
+        lp.setCombinerClass(HighestValuePagePerUser.class);
+        lp.setReducerClass(HighestValuePagePerUser.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+user+"/highest_value_page_per_user"));
+        lp.setNumReduceTasks(40);
+        Job loadPages = new Job(lp);
+
+        JobConf lu = new JobConf(L12.class);
+        lu.setJobName("Find Total Timespent per Term");
+        lu.setInputFormat(TextInputFormat.class);
+        lu.setOutputKeyClass(Text.class);
+        lu.setOutputValueClass(LongWritable.class);
+        lu.setMapperClass(TotalTimespentPerTerm.class);
+        lu.setCombinerClass(TotalTimespentPerTerm.class);
+        lu.setReducerClass(TotalTimespentPerTerm.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lu.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lu, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lu, new Path("/user/"+user+"/total_timespent_per_term"));
+        lu.setNumReduceTasks(40);
+        Job loadUsers = new Job(lu);
+
+        JobConf join = new JobConf(L12.class);
+        join.setJobName("Find Queries Per Action");
+        join.setInputFormat(TextInputFormat.class);
+        join.setOutputKeyClass(Text.class);
+        join.setOutputValueClass(LongWritable.class);
+        join.setMapperClass(QueriesPerAction.class);
+        join.setCombinerClass(QueriesPerAction.class);
+        join.setReducerClass(QueriesPerAction.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            join.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(join, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(join, new Path("/user/"+user+"/queries_per_action"));
+        join.setNumReduceTasks(40);
+        Job joinJob = new Job(join);
+
+        JobControl jc = new JobControl("L12 join");
+        jc.addJob(loadPages);
+        jc.addJob(loadUsers);
+        jc.addJob(joinJob);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L13.java test/org/apache/pig/test/pigmix/mapreduce/L13.java
new file mode 100644
index 0000000..5246191
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L13.java
@@ -0,0 +1,218 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.KeyValueTextInputFormat;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapred.lib.IdentityMapper;
+
+public class L13 {
+
+    public static class ReadLeftPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            List<Text> fields = Library.splitLine(val, '');
+            // Prepend an index to the value so we know which file
+            // it came from.
+            oc.collect(fields.get(0), new Text("1" + fields.get(6)));
+        }
+    }
+
+    public static class ReadRightPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            // Prepend an index to the value so we know which file
+            // it came from.
+            List<Text> fields = Library.splitLine(val, '\u0001');
+            if (fields.size()==0) return;
+            oc.collect(fields.get(0), new Text("2"+fields.get(1)));
+
+        }
+    }
+
+    public static class Join extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            // For each value, figure out which file it's from and store it
+            // accordingly.
+            List<String> first = new ArrayList<String>();
+            List<String> second = new ArrayList<String>();
+
+            while (iter.hasNext()) {
+                Text txt = iter.next();
+                if (txt.charAt(0) == '1') {
+                    first.add(txt.toString().substring(1));
+                }
+                else second.add(txt.toString().substring(1));
+                reporter.setStatus("OK");
+            }
+
+            reporter.setStatus("OK");
+
+            if (first.size() == 0) return;
+            if (second.size() == 0) second.add(null);
+
+            // Do the cross product
+            for (String s1 : first) {
+                for (String s2 : second) {
+                    if (s2==null) oc.collect(null, new Text(key + "\t" + s1 + "\t\t"));
+                    else oc.collect(null, new Text(key + "\t" + s1 + "\t" + key + "\t" + s2));
+                }
+            }
+            first.clear();
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        String user = System.getProperty("user.name");
+        JobConf lp = new JobConf(L13.class);
+        lp.setJobName("Load Left Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadLeftPageViews.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+user+"/tmp/indexed_left_pages"));
+        lp.setNumReduceTasks(0);
+        Job loadPages = new Job(lp);
+
+        JobConf lu = new JobConf(L13.class);
+        lu.setJobName("Load Right Page Views");
+        lu.setInputFormat(TextInputFormat.class);
+        lu.setOutputKeyClass(Text.class);
+        lu.setOutputValueClass(Text.class);
+        lu.setMapperClass(ReadRightPageViews.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lu.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lu, new Path(dataDir, "power_users_samples"));
+        FileOutputFormat.setOutputPath(lu, new Path("/user/"+user+"/tmp/indexed_right_pages"));
+        lu.setNumReduceTasks(0);
+        Job loadUsers = new Job(lu);
+
+        JobConf join = new JobConf(L13.class);
+        join.setJobName("Join Two Pages");
+        join.setInputFormat(KeyValueTextInputFormat.class);
+        join.setOutputKeyClass(Text.class);
+        join.setOutputValueClass(Text.class);
+        join.setMapperClass(IdentityMapper.class);
+        join.setReducerClass(Join.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            join.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_left_pages"));
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_right_pages"));
+        FileOutputFormat.setOutputPath(join, new Path("/user/"+user+"/L13out"));
+        join.setNumReduceTasks(40);
+        Job joinJob = new Job(join);
+        joinJob.addDependingJob(loadPages);
+        joinJob.addDependingJob(loadUsers);
+
+        JobControl jc = new JobControl("L13 join");
+        jc.addJob(loadPages);
+        jc.addJob(loadUsers);
+        jc.addJob(joinJob);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L14.java test/org/apache/pig/test/pigmix/mapreduce/L14.java
new file mode 100644
index 0000000..f85dc27
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L14.java
@@ -0,0 +1,220 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.KeyValueTextInputFormat;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapred.lib.IdentityMapper;
+
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L14 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            List<Text> fields = Library.splitLine(val, '');
+            // Prepend an index to the value so we know which file
+            // it came from.
+            Text outVal = new Text("1" + fields.get(6).toString());
+            oc.collect(fields.get(0), outVal);
+        }
+    }
+
+    public static class ReadUsers extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            List<Text> fields = Library.splitLine(val, '\u0001');
+            // Prepend an index to the value so we know which file
+            // it came from.
+            Text outVal = new Text("2");
+            oc.collect(fields.get(0), outVal);
+
+        }
+    }
+
+    public static class Join extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            // For each value, figure out which file it's from and store it
+            // accordingly.
+            List<String> first = new ArrayList<String>();
+            List<String> second = new ArrayList<String>();
+
+            while (iter.hasNext()) {
+                Text t = iter.next();
+                String value = t.toString();
+                if (value.charAt(0) == '1') first.add(value.substring(1));
+                else second.add(value.substring(1));
+                reporter.setStatus("OK");
+            }
+
+            reporter.setStatus("OK");
+
+            if (first.size() == 0 || second.size() == 0) return;
+
+            // Do the cross product, and calculate the sum
+            for (String s1 : first) {
+                for (String s2 : second) {
+                    try {
+                        oc.collect(null, new Text(key.toString() + "\t" + s1 + "\t" + key.toString()));
+                    } catch (NumberFormatException nfe) {
+                    }
+                }
+            }
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        String user = System.getProperty("user.name");
+        JobConf lp = new JobConf(L14.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views_sorted"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+user+"/tmp/indexed_pages"));
+        lp.setNumReduceTasks(0);
+        Job loadPages = new Job(lp);
+
+        JobConf lu = new JobConf(L14.class);
+        lu.setJobName("Load Users");
+        lu.setInputFormat(TextInputFormat.class);
+        lu.setOutputKeyClass(Text.class);
+        lu.setOutputValueClass(Text.class);
+        lu.setMapperClass(ReadUsers.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lu.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lu, new Path(dataDir, "users_sorted"));
+        FileOutputFormat.setOutputPath(lu, new Path("/user/"+user+"/tmp/indexed_users"));
+        lu.setNumReduceTasks(0);
+        Job loadUsers = new Job(lu);
+
+        JobConf join = new JobConf(L14.class);
+        join.setJobName("Join Users and Pages");
+        join.setInputFormat(KeyValueTextInputFormat.class);
+        join.setOutputKeyClass(Text.class);
+        join.setOutputValueClass(Text.class);
+        join.setMapperClass(IdentityMapper.class);
+        join.setReducerClass(Join.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            join.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_pages"));
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_users"));
+        FileOutputFormat.setOutputPath(join, new Path("/user/"+user+"/L14out"));
+        join.setNumReduceTasks(40);
+        Job joinJob = new Job(join);
+        joinJob.addDependingJob(loadPages);
+        joinJob.addDependingJob(loadUsers);
+
+        JobControl jc = new JobControl("L14 join");
+        jc.addJob(loadPages);
+        jc.addJob(loadUsers);
+        jc.addJob(joinJob);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L15.java test/org/apache/pig/test/pigmix/mapreduce/L15.java
new file mode 100644
index 0000000..50cbf5c
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L15.java
@@ -0,0 +1,209 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L15 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            StringBuffer sb = new StringBuffer();
+			sb.append(fields.get(1).toString()); // action
+            sb.append('');
+			sb.append(fields.get(6).toString()); // estimated_revenue
+            sb.append('');
+			sb.append(fields.get(2).toString()); // timespent
+            sb.append('');
+            oc.collect(fields.get(0), new Text(sb.toString()));
+        }
+    }
+
+    public static class Combiner extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            HashSet<Text> hash1 = new HashSet<Text>();
+            HashSet<Text> hash2 = new HashSet<Text>();
+            HashSet<Text> hash3 = new HashSet<Text>();
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+					hash1.add(vals.get(0));
+					hash2.add(vals.get(1));
+					hash3.add(vals.get(2));
+				} catch(NumberFormatException nfe) {
+				}
+			}
+			Double rev= new Double(0.0);
+			Integer ts=0;
+			try {
+            for (Text t : hash2) rev += Double.valueOf(t.toString());
+            for (Text t : hash3) ts += Integer.valueOf(t.toString());
+			} catch (NumberFormatException e) {
+			}
+			StringBuffer sb = new StringBuffer();
+			sb.append((new Integer(hash1.size())).toString());
+            sb.append("");
+            sb.append(rev.toString());
+            sb.append("");
+            sb.append(ts.toString());
+            sb.append("");
+            oc.collect(key, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            HashSet<Text> hash1 = new HashSet<Text>();
+            HashSet<Text> hash2 = new HashSet<Text>();
+            HashSet<Text> hash3 = new HashSet<Text>();
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+					hash1.add(vals.get(0));
+					hash2.add(vals.get(1));
+					hash3.add(vals.get(2));
+				}catch(NumberFormatException nfe) {
+				}
+			}
+			Integer ts=0;
+			Double rev=new Double(0.0);
+            for (Text t : hash2) rev += Double.valueOf(t.toString());
+            for (Text t : hash3) ts += Integer.valueOf(t.toString());
+            StringBuffer sb = new StringBuffer();
+			sb.append((new Integer(hash1.size())).toString());
+            sb.append("");
+            sb.append(rev.toString());
+            sb.append("");
+            sb.append(ts.toString());
+            oc.collect(key, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L15.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(Combiner.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L15out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L15 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L16.java test/org/apache/pig/test/pigmix/mapreduce/L16.java
new file mode 100644
index 0000000..df94a2b
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L16.java
@@ -0,0 +1,151 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.TreeSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.Properties;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L16 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            oc.collect(fields.get(0), fields.get(6));
+        }
+    }
+
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            Set<Double> hash2 = new TreeSet<Double>();
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+                   hash2.add(Double.valueOf(vals.get(0).toString()));
+                }catch(NumberFormatException nfe) {
+                }
+            }
+            Double rev=new Double(0);
+            for (Double d : hash2) rev += d;
+            oc.collect(key, new Text(rev.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L16.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L16out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L16 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L17.java test/org/apache/pig/test/pigmix/mapreduce/L17.java
new file mode 100644
index 0000000..5bb6620
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L17.java
@@ -0,0 +1,277 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L17 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            List<Text> vals = Library.splitLine(val, '');
+            if (vals.size() != 27) return;
+            StringBuffer key = new StringBuffer();
+            key.append(vals.get(0).toString());
+            key.append("");
+            key.append(vals.get(1).toString());
+            key.append("");
+            key.append(vals.get(2).toString());
+            key.append("");
+            key.append(vals.get(3).toString());
+            key.append("");
+            key.append(vals.get(4).toString());
+            key.append("");
+            key.append(vals.get(5).toString());
+            key.append("");
+            key.append(vals.get(6).toString());
+            key.append("");
+            key.append(vals.get(9).toString());
+            key.append("");
+            key.append(vals.get(10).toString());
+            key.append("");
+            key.append(vals.get(11).toString());
+            key.append("");
+            key.append(vals.get(12).toString());
+            key.append("");
+            key.append(vals.get(13).toString());
+            key.append("");
+            key.append(vals.get(14).toString());
+            key.append("");
+            key.append(vals.get(15).toString());
+            key.append("");
+            key.append(vals.get(18).toString());
+            key.append("");
+            key.append(vals.get(19).toString());
+            key.append("");
+            key.append(vals.get(20).toString());
+            key.append("");
+            key.append(vals.get(21).toString());
+            key.append("");
+            key.append(vals.get(22).toString());
+            key.append("");
+            key.append(vals.get(23).toString());
+            key.append("");
+            key.append(vals.get(24).toString());
+
+            StringBuffer sb = new StringBuffer();
+            sb.append(vals.get(2).toString());
+            sb.append("");
+            sb.append(vals.get(11).toString());
+            sb.append("");
+            sb.append(vals.get(20).toString());
+            sb.append("");
+            sb.append(vals.get(6).toString());
+            sb.append("");
+            sb.append(vals.get(15).toString());
+            sb.append("");
+            sb.append(vals.get(24).toString());
+            oc.collect(new Text(key.toString()), new Text(sb.toString()));
+        }
+    }
+
+    public static class Combiner extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            int tsSum = 0, tsSum1 = 0, tsSum2 = 0, erCnt = 0;
+            double erSum = 0.0, erSum1 = 0.0, erSum2=0.0;
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+                    tsSum += Integer.valueOf(vals.get(0).toString());
+                } catch (NumberFormatException nfe) {
+                }
+                try {
+                    tsSum1 += Integer.valueOf(vals.get(1).toString());
+                } catch (NumberFormatException nfe) {
+                }
+                try {
+                    tsSum2 += Integer.valueOf(vals.get(2).toString());
+                } catch (NumberFormatException nfe) {
+                }
+                try {
+                    erSum += Double.valueOf(vals.get(3).toString());
+                } catch (NumberFormatException nfe) {
+                }
+                try {
+                    erSum1 += Double.valueOf(vals.get(4).toString());
+                } catch (NumberFormatException nfe) {
+                }
+                try {
+                    erSum2 += Double.valueOf(vals.get(5).toString());
+                } catch (NumberFormatException nfe) {
+                }
+                erCnt++;
+            }
+            StringBuffer sb = new StringBuffer();
+            sb.append((new Integer(tsSum)).toString());
+            sb.append("");
+            sb.append((new Integer(tsSum1)).toString());
+            sb.append("");
+            sb.append((new Integer(tsSum2)).toString());
+            sb.append("");
+            sb.append((new Double(erSum)).toString());
+            sb.append("");
+            sb.append((new Double(erSum1)).toString());
+            sb.append("");
+            sb.append((new Double(erSum2)).toString());
+            sb.append("");
+            sb.append((new Integer(erCnt)).toString());
+            oc.collect(key, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            int tsSum = 0, tsSum1 = 0, tsSum2 = 0, erCnt = 0;
+            double erSum = 0.0, erSum1 = 0.0, erSum2 = 0.0;
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+                    tsSum += Integer.valueOf(vals.get(0).toString());
+                    tsSum1 += Integer.valueOf(vals.get(1).toString());
+                    tsSum2 += Integer.valueOf(vals.get(2).toString());
+                    erSum += Double.valueOf(vals.get(3).toString());
+                    erSum1 += Double.valueOf(vals.get(4).toString());
+                    erSum2 += Double.valueOf(vals.get(5).toString());
+                    erCnt++;
+                } catch (NumberFormatException nfe) {
+                }
+            }
+            double erAvg = erSum / erCnt, erAvg1 = erSum1 / erCnt, erAvg2 = erSum2 / erCnt;
+            StringBuffer sb = new StringBuffer();
+            sb.append((new Double(tsSum)).toString());
+            sb.append("\t");
+            sb.append((new Double(tsSum1)).toString());
+            sb.append("\t");
+            sb.append((new Double(tsSum2)).toString());
+            sb.append("\t");
+            sb.append((new Double(erAvg)).toString());
+            sb.append("\t");
+            sb.append((new Double(erAvg1)).toString());
+            sb.append("\t");
+            sb.append((new Double(erAvg2)).toString());
+            oc.collect(null, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L17.class);
+        lp.setJobName("Wide group by");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(Combiner.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "widegroupbydata"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L17out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L17 group by");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L2.java test/org/apache/pig/test/pigmix/mapreduce/L2.java
new file mode 100644
index 0000000..5bc7ecc
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L2.java
@@ -0,0 +1,165 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.BufferedReader;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.filecache.DistributedCache;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L2 {
+
+    public static class Join extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        Set<String> hash;
+
+        @Override
+        public void configure(JobConf conf) {
+            try {
+                Path[] paths = DistributedCache.getLocalCacheFiles(conf);
+                if (paths == null || paths.length < 1) {
+                    throw new RuntimeException("DistributedCache no work.");
+                }
+
+                // Open the small table
+                BufferedReader reader =
+                    new BufferedReader(new InputStreamReader(new
+                    FileInputStream(paths[0].toString())));
+                String line;
+                hash = new HashSet<String>(500);
+                while ((line = reader.readLine()) != null) {
+                    if (line.length() < 1) continue;
+                    String[] fields = line.split("");
+                    hash.add(fields[0]);
+                }
+            } catch (IOException ioe) {
+                throw new RuntimeException(ioe);
+            }
+        }
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            List<Text> fields = Library.splitLine(val, '');
+            String name = fields.get(0).toString();
+
+            String v;
+            if (hash.contains(name)) {
+                StringBuffer sb = new StringBuffer();
+                sb.append(name);
+                sb.append("");
+                sb.append(fields.get(6).toString());
+                oc.collect(null, new Text(sb.toString()));
+            }
+
+        }
+    }
+
+    public static void main(String[] args) throws IOException, URISyntaxException {
+
+        JobConf lp = new JobConf(L2.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(Join.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        DistributedCache.addCacheFile(
+            new URI(dataDir + "/power_users"), lp);
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L2out"));
+        lp.setNumReduceTasks(0);
+        Job loadPages = new Job(lp);
+
+        JobControl jc = new JobControl("L2 join");
+        jc.addJob(loadPages);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L3.java test/org/apache/pig/test/pigmix/mapreduce/L3.java
new file mode 100644
index 0000000..f7d6334
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L3.java
@@ -0,0 +1,222 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.KeyValueTextInputFormat;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapred.lib.IdentityMapper;
+
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L3 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            List<Text> fields = Library.splitLine(val, '');
+            // Prepend an index to the value so we know which file
+            // it came from.
+            Text outVal = new Text("1" + fields.get(6).toString());
+            oc.collect(fields.get(0), outVal);
+        }
+    }
+
+    public static class ReadUsers extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            List<Text> fields = Library.splitLine(val, '');
+            // Prepend an index to the value so we know which file
+            // it came from.
+            Text outVal = new Text("2");
+            oc.collect(fields.get(0), outVal);
+
+        }
+    }
+
+    public static class Join extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            // For each value, figure out which file it's from and store it
+            // accordingly.
+            List<String> first = new ArrayList<String>();
+            List<String> second = new ArrayList<String>();
+
+            while (iter.hasNext()) {
+                Text t = iter.next();
+                String value = t.toString();
+                if (value.charAt(0) == '1') first.add(value.substring(1));
+                else second.add(value.substring(1));
+                reporter.setStatus("OK");
+            }
+
+            reporter.setStatus("OK");
+
+            if (first.size() == 0 || second.size() == 0) return;
+
+            // Do the cross product, and calculate the sum
+            Double sum = 0.0;
+            for (String s1 : first) {
+                for (String s2 : second) {
+                    try {
+                        sum += Double.valueOf(s1);
+                    } catch (NumberFormatException nfe) {
+                    }
+                }
+            }
+            oc.collect(null, new Text(key.toString() + "" +  sum.toString()));
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        String user = System.getProperty("user.name");
+        JobConf lp = new JobConf(L3.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+user+"/tmp/indexed_pages"));
+        lp.setNumReduceTasks(0);
+        Job loadPages = new Job(lp);
+
+        JobConf lu = new JobConf(L3.class);
+        lu.setJobName("Load Users");
+        lu.setInputFormat(TextInputFormat.class);
+        lu.setOutputKeyClass(Text.class);
+        lu.setOutputValueClass(Text.class);
+        lu.setMapperClass(ReadUsers.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lu.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lu, new Path(dataDir, "users"));
+        FileOutputFormat.setOutputPath(lu, new Path("/user/"+user+"/tmp/indexed_users"));
+        lu.setNumReduceTasks(0);
+        Job loadUsers = new Job(lu);
+
+        JobConf join = new JobConf(L3.class);
+        join.setJobName("Join Users and Pages");
+        join.setInputFormat(KeyValueTextInputFormat.class);
+        join.setOutputKeyClass(Text.class);
+        join.setOutputValueClass(Text.class);
+        join.setMapperClass(IdentityMapper.class);
+        join.setReducerClass(Join.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            join.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_pages"));
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_users"));
+        FileOutputFormat.setOutputPath(join, new Path("/user/"+user+"/L3out"));
+        join.setNumReduceTasks(40);
+        Job joinJob = new Job(join);
+        joinJob.addDependingJob(loadPages);
+        joinJob.addDependingJob(loadUsers);
+
+        JobControl jc = new JobControl("L3 join");
+        jc.addJob(loadPages);
+        jc.addJob(loadUsers);
+        jc.addJob(joinJob);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L4.java test/org/apache/pig/test/pigmix/mapreduce/L4.java
new file mode 100644
index 0000000..6ba799d
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L4.java
@@ -0,0 +1,162 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L4 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            oc.collect(fields.get(0), fields.get(1));
+        }
+    }
+
+    public static class Combiner extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            HashSet<Text> hash = new HashSet<Text>();
+            while (iter.hasNext()) {
+                hash.add(iter.next());
+            }
+            for (Text t : hash) oc.collect(key, t);
+            reporter.setStatus("OK");
+        }
+    }
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            HashSet<Text> hash = new HashSet<Text>();
+            while (iter.hasNext()) {
+                hash.add(iter.next());
+            }
+            oc.collect(key, new Text(Integer.toString(hash.size())));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L4.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(Combiner.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L4out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L4 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L5.java test/org/apache/pig/test/pigmix/mapreduce/L5.java
new file mode 100644
index 0000000..c8f083f
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L5.java
@@ -0,0 +1,205 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.KeyValueTextInputFormat;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.hadoop.mapred.lib.IdentityMapper;
+
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L5 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Pull the key out
+            List<Text> line = Library.splitLine(val, '');
+            // Prepend an index to the value so we know which file
+            // it came from.
+            oc.collect(line.get(0), new Text("1"));
+        }
+    }
+
+    public static class ReadUsers extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Pull the key out
+            List<Text> line = Library.splitLine(val, '');
+            // Prepend an index to the value so we know which file
+            // it came from.
+            oc.collect(line.get(0), new Text("2"));
+        }
+    }
+
+    public static class Join extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            // For each value, figure out which file it's from and store it
+            // accordingly.
+            int cnt = 0;
+
+            while (iter.hasNext()) {
+                if (iter.next().toString().charAt(0) == '2') cnt++;
+                reporter.setStatus("OK");
+            }
+
+            oc.collect(null, key);
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        String user = System.getProperty("user.name");
+        JobConf lp = new JobConf(L5.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+user+"/tmp/indexed_pages"));
+        lp.setNumReduceTasks(0);
+        Job loadPages = new Job(lp);
+
+        JobConf lu = new JobConf(L5.class);
+        lu.setJobName("Load Users");
+        lu.setInputFormat(TextInputFormat.class);
+        lu.setOutputKeyClass(Text.class);
+        lu.setOutputValueClass(Text.class);
+        lu.setMapperClass(ReadUsers.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lu.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lu, new Path(dataDir, "users"));
+        FileOutputFormat.setOutputPath(lu, new Path("/user/"+user+"/tmp/indexed_users"));
+        lu.setNumReduceTasks(0);
+        Job loadUsers = new Job(lu);
+
+        JobConf join = new JobConf(L5.class);
+        join.setJobName("Join Users and Pages");
+        join.setInputFormat(KeyValueTextInputFormat.class);
+        join.setOutputKeyClass(Text.class);
+        join.setOutputValueClass(Text.class);
+        join.setMapperClass(IdentityMapper.class);
+        join.setReducerClass(Join.class);
+        props = System.getProperties();
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            join.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_pages"));
+        FileInputFormat.addInputPath(join, new Path("/user/"+user+"/tmp/indexed_users"));
+        FileOutputFormat.setOutputPath(join, new Path("/user/"+user+"/L5out"));
+        join.setNumReduceTasks(40);
+        Job joinJob = new Job(join);
+        joinJob.addDependingJob(loadPages);
+        joinJob.addDependingJob(loadUsers);
+
+        JobControl jc = new JobControl("L5 join");
+        jc.addJob(loadPages);
+        jc.addJob(loadUsers);
+        jc.addJob(joinJob);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L6.java test/org/apache/pig/test/pigmix/mapreduce/L6.java
new file mode 100644
index 0000000..9b87b93
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L6.java
@@ -0,0 +1,159 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.IntWritable;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L6 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, IntWritable> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, IntWritable> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            StringBuffer sb = new StringBuffer();
+            sb.append(fields.get(0).toString());
+            sb.append("");
+            sb.append(fields.get(3).toString());
+            sb.append("");
+            sb.append(fields.get(4).toString());
+            sb.append("");
+            sb.append(fields.get(5).toString());
+            Text key = new Text(sb.toString());
+
+            try {
+                oc.collect(fields.get(0),
+                    new IntWritable(Integer.valueOf(fields.get(3).toString())));
+            } catch (NumberFormatException nfe) {
+            }
+        }
+    }
+
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, IntWritable, Text, IntWritable> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<IntWritable> iter,
+                OutputCollector<Text, IntWritable> oc,
+                Reporter reporter) throws IOException {
+            int sum = 0;
+            while (iter.hasNext()) {
+                sum += iter.next().get();
+            }
+            oc.collect(key, new IntWritable(sum));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L6.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(IntWritable.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(Group.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L6out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L6 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L7.java test/org/apache/pig/test/pigmix/mapreduce/L7.java
new file mode 100644
index 0000000..92e799f
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L7.java
@@ -0,0 +1,179 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L7 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            oc.collect(fields.get(0), fields.get(5));
+        }
+    }
+
+    public static class Combiner extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            int morning = 0, afternoon = 0;
+            while (iter.hasNext()) {
+                try {
+                    if (Integer.valueOf(iter.next().toString()) < 43200) morning++;
+                    else afternoon++;
+                } catch (NumberFormatException nfe) {
+                }
+            }
+            StringBuffer sb = new StringBuffer();
+            sb.append((new Integer(morning)).toString());
+            sb.append("");
+            sb.append((new Integer(afternoon)).toString());
+            oc.collect(key, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            int morning = 0, afternoon = 0;
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+                    morning += Integer.valueOf(vals.get(0).toString());
+                    if (vals.size() > 1) afternoon += Integer.valueOf(vals.get(1).toString());
+                } catch (NumberFormatException nfe) {
+                }
+            }
+            StringBuffer sb = new StringBuffer();
+            sb.append((new Integer(morning)).toString());
+            sb.append("");
+            sb.append((new Integer(afternoon)).toString());
+            oc.collect(key, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L7.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(Combiner.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L7out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L7 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L8.java test/org/apache/pig/test/pigmix/mapreduce/L8.java
new file mode 100644
index 0000000..5abbe59
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L8.java
@@ -0,0 +1,190 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.Map;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L8 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            StringBuffer sb = new StringBuffer();
+            sb.append(fields.get(2).toString());
+            sb.append("");
+            sb.append(fields.get(6).toString());
+            oc.collect(new Text("all"), new Text(sb.toString()));
+        }
+    }
+
+    public static class Combiner extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            int tsSum = 0, erCnt = 0;
+            double erSum = 0.0;
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+                    tsSum += Integer.valueOf(vals.get(0).toString());
+                    erSum += Double.valueOf(vals.get(1).toString());
+                    erCnt++;
+                } catch (NumberFormatException nfe) {
+                }
+            }
+            StringBuffer sb = new StringBuffer();
+            sb.append((new Integer(tsSum)).toString());
+            sb.append("");
+            sb.append((new Double(erSum)).toString());
+            sb.append("");
+            sb.append((new Integer(erCnt)).toString());
+            oc.collect(key, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            int tsSum = 0, erCnt = 0;
+            double erSum = 0.0;
+            while (iter.hasNext()) {
+                List<Text> vals = Library.splitLine(iter.next(), '');
+                try {
+                    tsSum += Integer.valueOf(vals.get(0).toString());
+                    erSum += Double.valueOf(vals.get(1).toString());
+                    erCnt++;
+                } catch (NumberFormatException nfe) {
+                }
+            }
+            double erAvg = erSum / erCnt;
+            StringBuffer sb = new StringBuffer();
+            sb.append((new Integer(tsSum)).toString());
+            sb.append("");
+            sb.append((new Double(erAvg)).toString());
+            oc.collect(key, new Text(sb.toString()));
+            reporter.setStatus("OK");
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L8.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setCombinerClass(Combiner.class);
+        lp.setReducerClass(Group.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L8out"));
+        lp.setNumReduceTasks(1);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L8 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/L9.java test/org/apache/pig/test/pigmix/mapreduce/L9.java
new file mode 100644
index 0000000..c6436bd
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/L9.java
@@ -0,0 +1,224 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Properties;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.io.Text;
+import org.apache.hadoop.io.LongWritable;
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.FileOutputFormat;
+import org.apache.hadoop.mapred.JobConf;
+import org.apache.hadoop.mapred.Mapper;
+import org.apache.hadoop.mapred.MapReduceBase;
+import org.apache.hadoop.mapred.OutputCollector;
+import org.apache.hadoop.mapred.Partitioner;
+import org.apache.hadoop.mapred.Reducer;
+import org.apache.hadoop.mapred.Reporter;
+import org.apache.hadoop.mapred.TextInputFormat;
+import org.apache.hadoop.mapred.jobcontrol.Job;
+import org.apache.hadoop.mapred.jobcontrol.JobControl;
+import org.apache.pig.test.pigmix.mapreduce.Library;
+
+public class L9 {
+
+    public static class ReadPageViews extends MapReduceBase
+        implements Mapper<LongWritable, Text, Text, Text> {
+
+        @Override
+        public void map(
+                LongWritable k,
+                Text val,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+
+            // Split the line
+            List<Text> fields = Library.splitLine(val, '');
+            if (fields.size() != 9) return;
+
+            oc.collect(fields.get(0), val);
+        }
+    }
+
+    public static class MyPartitioner implements Partitioner<Text, Text> {
+
+        public Map<Character, Integer> map;
+
+        @Override
+        public int getPartition(Text key, Text value, int numPartitions) {
+            int rc = 0;
+            String s = key.toString();
+            if (s == null || s.length() < 2) return 39;
+            if (s.charAt(0) > ']') rc += 20;
+            rc += map.get(s.charAt(1));
+            return rc;
+        }
+
+        @Override
+        public void configure(JobConf conf) {
+            // Don't actually do any configuration, do the setup of the hash
+            // because this call is guaranteed to be made each time we set up
+            // MyPartitioner
+            map = new HashMap<Character, Integer>(57);
+            map.put('A', 0);
+            map.put('B', 1);
+            map.put('C', 2);
+            map.put('D', 3);
+            map.put('E', 4);
+            map.put('F', 5);
+            map.put('G', 6);
+            map.put('I', 7);
+            map.put('H', 8);
+            map.put('J', 9);
+            map.put('K', 10);
+            map.put('L', 11);
+            map.put('M', 12);
+            map.put('N', 13);
+            map.put('O', 14);
+            map.put('P', 15);
+            map.put('Q', 16);
+            map.put('R', 17);
+            map.put('S', 18);
+            map.put('T', 19);
+            map.put('U', 0);
+            map.put('V', 1);
+            map.put('W', 2);
+            map.put('X', 3);
+            map.put('Y', 4);
+            map.put('Z', 5);
+            map.put('[', 6);
+            map.put('\\', 7);
+            map.put(']', 8);
+            map.put('^', 9);
+            map.put('_', 10);
+            map.put('`', 11);
+            map.put('a', 12);
+            map.put('b', 13);
+            map.put('c', 14);
+            map.put('d', 15);
+            map.put('e', 16);
+            map.put('f', 17);
+            map.put('g', 18);
+            map.put('h', 19);
+            map.put('i', 0);
+            map.put('j', 1);
+            map.put('k', 2);
+            map.put('l', 3);
+            map.put('m', 4);
+            map.put('n', 5);
+            map.put('o', 6);
+            map.put('p', 7);
+            map.put('q', 8);
+            map.put('r', 9);
+            map.put('s', 10);
+            map.put('t', 11);
+            map.put('u', 12);
+            map.put('v', 13);
+            map.put('w', 14);
+            map.put('x', 15);
+            map.put('y', 16);
+            map.put('z', 17);
+        }
+    }
+
+    public static class Group extends MapReduceBase
+        implements Reducer<Text, Text, Text, Text> {
+
+        @Override
+        public void reduce(
+                Text key,
+                Iterator<Text> iter,
+                OutputCollector<Text, Text> oc,
+                Reporter reporter) throws IOException {
+            while (iter.hasNext()) {
+                oc.collect(null, iter.next());
+            }
+        }
+    }
+
+    public static void main(String[] args) throws IOException {
+
+        JobConf lp = new JobConf(L9.class);
+        lp.setJobName("Load Page Views");
+        lp.setInputFormat(TextInputFormat.class);
+        lp.setOutputKeyClass(Text.class);
+        lp.setOutputValueClass(Text.class);
+        lp.setMapperClass(ReadPageViews.class);
+        lp.setReducerClass(Group.class);
+        lp.setPartitionerClass(MyPartitioner.class);
+        Properties props = System.getProperties();
+        String dataDir = props.getProperty("PIGMIX_DIR", "/user/pig/tests/data/pigmix");
+        for (Map.Entry<Object,Object> entry : props.entrySet()) {
+            lp.set((String)entry.getKey(), (String)entry.getValue());
+        }
+        FileInputFormat.addInputPath(lp, new Path(dataDir, "page_views"));
+        FileOutputFormat.setOutputPath(lp, new Path("/user/"+System.getProperty("user.name")+"/L9out"));
+        lp.setNumReduceTasks(40);
+        Job group = new Job(lp);
+
+        JobControl jc = new JobControl("L9 join");
+        jc.addJob(group);
+
+        new Thread(jc).start();
+
+        int i = 0;
+        while(!jc.allFinished()){
+            ArrayList<Job> failures = jc.getFailedJobs();
+            if (failures != null && failures.size() > 0) {
+                for (Job failure : failures) {
+                    System.err.println(failure.getMessage());
+                }
+                break;
+            }
+
+            try {
+                Thread.sleep(5000);
+            } catch (InterruptedException e) {}
+
+            if (i % 10000 == 0) {
+                System.out.println("Running jobs");
+                ArrayList<Job> running = jc.getRunningJobs();
+                if (running != null && running.size() > 0) {
+                    for (Job r : running) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Ready jobs");
+                ArrayList<Job> ready = jc.getReadyJobs();
+                if (ready != null && ready.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Waiting jobs");
+                ArrayList<Job> waiting = jc.getWaitingJobs();
+                if (waiting != null && waiting.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+                System.out.println("Successful jobs");
+                ArrayList<Job> success = jc.getSuccessfulJobs();
+                if (success != null && success.size() > 0) {
+                    for (Job r : ready) {
+                        System.out.println(r.getJobName());
+                    }
+                }
+            }
+            i++;
+        }
+        ArrayList<Job> failures = jc.getFailedJobs();
+        if (failures != null && failures.size() > 0) {
+            for (Job failure : failures) {
+                System.err.println(failure.getMessage());
+            }
+        }
+        jc.stop();
+    }
+
+}
diff --git test/org/apache/pig/test/pigmix/mapreduce/Library.java test/org/apache/pig/test/pigmix/mapreduce/Library.java
new file mode 100644
index 0000000..71fd8f2
--- /dev/null
+++ test/org/apache/pig/test/pigmix/mapreduce/Library.java
@@ -0,0 +1,45 @@
+package org.apache.pig.test.pigmix.mapreduce;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.hadoop.io.Text;
+
+/**
+ * A collection of static functions for use by the pigmix map reduce tasks.
+ */
+public class Library {
+
+    public static List<Text> splitLine(Text line, char delimiter) {
+        String s = line.toString();
+        List<Text> cols = new ArrayList<Text>();
+        int start = 0;
+        for (int i = 0; i < s.length(); i++) {
+            if (s.charAt(i) == delimiter) {
+                if (start == i) cols.add(new Text()); // null case
+                else cols.add(new Text(s.substring(start, i)));
+                start = i + 1;
+            }
+        }
+        // Grab the last one.
+        if (start != s.length() - 1) cols.add(new Text(s.substring(start)));
+
+        return cols;
+    }
+
+    public static Text mapLookup(Text mapCol, Text key) {
+        List<Text> kvps = splitLine(mapCol, '');
+
+        for (Text potential : kvps) {
+            // Split potential on ^D
+            List<Text> kv = splitLine(potential, '');
+            if (kv.size() != 2) return null;
+            if (kv.get(0).equals(potential)) return kv.get(1);
+        }
+
+        return null;
+    }
+
+        
+                
+}
diff --git test/org/apache/pig/test/utils/datagen/DataGenerator.java test/org/apache/pig/test/utils/datagen/DataGenerator.java
new file mode 100644
index 0000000..53f6730
--- /dev/null
+++ test/org/apache/pig/test/utils/datagen/DataGenerator.java
@@ -0,0 +1,831 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.pig.test.utils.datagen;
+
+import java.io.*;
+import java.lang.SecurityException;
+import java.text.ParseException;
+import java.util.*;
+
+import sdsu.algorithms.data.Zipf;
+
+import org.apache.pig.tools.cmdline.CmdLineParser;
+
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.conf.*;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.mapred.*;
+import org.apache.hadoop.util.*;
+
+/**
+ * A tool to generate data for performance testing.
+ */
+public class DataGenerator extends Configured implements Tool {
+    ColSpec[] colSpecs;
+    long seed = -1;
+    long numRows = -1;
+    int numMappers = -1;
+    String outputFile;
+    String inFile;
+    char separator = '\u0001' ;
+    Random rand;
+
+    private String[] mapkey = { "a", "b", "c", "d", "e", "f", "g", "h", "i",
+        "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w",
+        "x", "y", "z"};
+
+    public static void main(String[] args) throws Exception {    	
+    	DataGenerator dg = new DataGenerator();    	 
+    	try {
+    		ToolRunner.run(new Configuration(), dg, args);    			
+   		}catch(Exception e) {
+    		throw new IOException (e);
+    	}    	
+        dg.go();
+    }
+    
+    protected DataGenerator(long seed) {    
+    	System.out.println("Using seed " + seed);
+        rand = new Random(seed);
+    }
+
+    protected DataGenerator() {
+    	
+    }
+    
+    protected DataGenerator(String[] args) {
+    	
+    }
+    
+    public int run(String[] args) throws Exception {    	
+        CmdLineParser opts = new CmdLineParser(args);
+        opts.registerOpt('e', "seed", CmdLineParser.ValueExpected.REQUIRED);
+        opts.registerOpt('f', "file", CmdLineParser.ValueExpected.REQUIRED);
+        opts.registerOpt('r', "rows", CmdLineParser.ValueExpected.REQUIRED);
+        opts.registerOpt('s', "separator", CmdLineParser.ValueExpected.REQUIRED);
+        opts.registerOpt('i', "input", CmdLineParser.ValueExpected.REQUIRED);
+        opts.registerOpt('m', "mappers", CmdLineParser.ValueExpected.OPTIONAL);
+
+        char opt;
+        try {
+            while ((opt = opts.getNextOpt()) != CmdLineParser.EndOfOpts) {
+                switch (opt) {
+                case 'e':
+                    seed = Long.valueOf(opts.getValStr());
+                    break;
+
+                case 'f':
+                    outputFile = opts.getValStr();                   
+                    break;
+
+                case 'i':
+                    inFile = opts.getValStr();                   
+                    break;
+
+                case 'r':
+                    numRows = Long.valueOf(opts.getValStr());
+                    break;
+    
+                case 's':
+                    separator = opts.getValStr().charAt(0);
+                    break;
+                    
+                case 'm':
+                	numMappers = Integer.valueOf(opts.getValStr());
+                	break;
+
+                default:
+                    usage();
+                    break;
+                }
+            }
+        } catch (ParseException pe) {
+            System.err.println("Couldn't parse the command line arguments, " +
+                pe.getMessage());
+            usage();
+        }
+
+        if (numRows < 1 && inFile == null) usage();
+
+        if (numRows > 0 && inFile != null) usage();              
+        
+        if (numMappers > 0 && seed != -1) usage();
+        
+        if (seed == -1){
+        	seed = System.currentTimeMillis();
+        }
+        
+        String remainders[] = opts.getRemainingArgs();
+        colSpecs = new ColSpec[remainders.length];
+        for (int i = 0; i < remainders.length; i++) {
+            colSpecs[i] = new ColSpec(remainders[i]);
+        }
+        System.err.println("Using seed " + seed);
+        rand = new Random(seed); 
+        
+        return 0;
+    }
+
+    private void go() throws IOException {    	    
+    	long t1 = System.currentTimeMillis();
+    	if (numMappers <= 0) {
+    		System.out.println("Generate data in local mode.");
+        	goLocal();
+        }else{
+        	System.out.println("Generate data in hadoop mode.");        
+        	HadoopRunner runner = new HadoopRunner();
+        	runner.goHadoop();
+        }
+    	   	
+    	long t2 = System.currentTimeMillis();
+    	System.out.println("Job is successful! It took " + (t2-t1)/1000 + " seconds.");
+    }
+    
+    public void goLocal() throws IOException {
+    	
+    	PrintWriter out = null;
+		try {
+            out = new PrintWriter(outputFile);
+        } catch (FileNotFoundException fnfe) {
+            System.err.println("Could not find file " + outputFile +
+                ", " + fnfe.getMessage());
+            return;
+        } catch (SecurityException se) {
+            System.err.println("Could not write to file " + outputFile +
+                ", " + se.getMessage());
+            return;
+        }
+        
+        BufferedReader in = null;
+        if (inFile != null) {
+        	try {
+      	  		in = new BufferedReader(new FileReader(inFile));
+        	} catch (FileNotFoundException fnfe) {
+        		System.err.println("Unable to find input file " + inFile);
+        		return;
+        	}
+        }
+        
+        if (numRows > 0) {
+        	for (int i = 0; i < numRows; i++) {
+        		writeLine(out);
+        		out.println();
+        	}
+        } else if (in != null) {
+        	String line;
+        	while ((line = in.readLine()) != null) {
+        		out.print(line);
+        		writeLine(out);
+        		out.println();
+        	}
+        }
+        out.close();
+    }      
+    
+    protected void writeLine(PrintWriter out) {
+        for (int j = 0; j < colSpecs.length; j++) {
+            if (j != 0) out.print(separator);
+            // First, decide if it's going to be null            
+            if (rand.nextInt(100) < colSpecs[j].pctNull) {
+            	continue;
+            }
+            writeCol(colSpecs[j], out);
+        }
+    }
+
+    private void writeCol(ColSpec colspec, PrintWriter out) {
+        switch (colspec.datatype) {
+        case INT:
+            out.print(colspec.nextInt());
+            break;
+
+        case LONG:
+            out.print(colspec.nextLong());
+            break;
+
+        case FLOAT:
+            out.print(colspec.nextFloat());
+            break;
+
+        case DOUBLE:
+            out.print(colspec.nextDouble());
+            break;
+
+        case STRING:
+            out.print(colspec.nextString());
+            break;
+
+        case MAP:
+            int len = rand.nextInt(20) + 6;
+            for (int k = 0; k < len; k++) {
+                if (k != 0) out.print('');
+                out.print(mapkey[k] + '');
+                out.print(colspec.gen.randomString());
+            }
+            break;
+
+        case BAG:
+            int numElements = rand.nextInt(5) + 5;
+            for (int i = 0; i < numElements; i++) {
+                if (i != 0) out.print('');
+                switch(colspec.contained.datatype) {
+                    case INT: out.print("i"); break;
+                    case LONG: out.print("l"); break;
+                    case FLOAT: out.print("f"); break;
+                    case DOUBLE: out.print("d"); break;
+                    case STRING: out.print("s"); break;
+                    case MAP: out.print("m"); break;
+                    case BAG: out.print("b"); break;
+                    default: throw new RuntimeException("should never be here");
+                }
+                writeCol(colspec.contained, out);
+            }
+        }
+    }
+
+    private void usage() {
+        System.err.println("Usage: datagen -rows numrows [options] colspec ...");
+        System.err.println("\tOptions:");
+        System.err.println("\t-e -seed seed value for random numbers");
+        System.err.println("\t-f -file output file, default is stdout");
+        System.err.println("\t-i -input input file, lines will be read from");
+        System.err.println("\t\tthe file and additional columns appended.");
+        System.err.println("\t\tMutually exclusive with -r.");
+        System.err.println("\t-r -rows number of rows to output");
+        System.err.println("\t-s -separator character, default is ^A");
+        System.err.println("\t-m -number of mappers to run concurrently to generate data. " +
+        		"If not specified, DataGenerator runs locally. This option can NOT be used with -e.");
+        System.err.println();
+        System.err.print("\tcolspec: columntype:average_size:cardinality:");
+        System.err.println("distribution_type:percent_null");
+        System.err.println("\tcolumntype:");
+        System.err.println("\t\ti = int");
+        System.err.println("\t\tl = long");
+        System.err.println("\t\tf = float");
+        System.err.println("\t\td = double");
+        System.err.println("\t\ts = string");
+        System.err.println("\t\tm = map");
+        System.err.println("\t\tbx = bag of x, where x is a columntype");
+        System.err.println("\tdistribution_type:");
+        System.err.println("\t\tu = uniform");
+        System.err.println("\t\tz = zipf");
+
+        throw new RuntimeException();
+    }
+
+
+ 
+    static enum Datatype { INT, LONG, FLOAT, DOUBLE, STRING, MAP, BAG };
+    static enum DistributionType { UNIFORM, ZIPF };
+    protected class ColSpec {
+    	String arg;
+        Datatype datatype;
+        DistributionType distype;
+        int avgsz;
+        int card;
+        RandomGenerator gen;
+        int pctNull;
+        ColSpec contained;
+        String mapfile;
+        Map<Integer, Object> map;
+
+        public ColSpec(String arg) {
+        	this.arg = arg;
+        	
+            String[] parts = arg.split(":");
+            if (parts.length != 5 && parts.length != 6) {
+                System.err.println("Colspec [" + arg + "] format incorrect"); 
+                usage();
+            }
+
+            switch (parts[0].charAt(0)) {
+                case 'i': datatype = Datatype.INT; break;
+                case 'l': datatype = Datatype.LONG; break;
+                case 'f': datatype = Datatype.FLOAT; break;
+                case 'd': datatype = Datatype.DOUBLE; break;
+                case 's': datatype = Datatype.STRING; break;
+                case 'm': datatype = Datatype.MAP; break;
+                case 'b':
+                    datatype = Datatype.BAG;
+                    contained = new ColSpec(arg.substring(1));
+                    return;
+                default: 
+                    System.err.println("Don't know column type " +
+                        parts[0].charAt(0));
+                    usage();
+                    break;
+            }
+            avgsz = Integer.valueOf(parts[1]);
+            card = Integer.valueOf(parts[2]);
+            switch (parts[3].charAt(0)) {
+                case 'u': 
+                    gen = new UniformRandomGenerator(avgsz, card);
+                    distype = DistributionType.UNIFORM;
+                    break;
+
+                case 'z': 
+                    gen = new ZipfRandomGenerator(avgsz, card);
+                    distype = DistributionType.ZIPF;
+                    break;
+
+                default:
+                    System.err.println("Don't know generator type " +
+                        parts[3].charAt(0));
+                    usage();
+                    break;
+            }
+
+            pctNull = Integer.valueOf(parts[4]);
+            if (pctNull > 100) {
+                System.err.println("Percentage null must be between 0-100, "
+                    + "you gave" + pctNull);
+                usage();
+            }
+            contained = null;
+
+            // if config has 6 columns, the last col is the file name 
+            // of the mapping file from random number to field value
+            if (parts.length == 6) {
+            	mapfile = parts[5];
+            	gen.hasMapFile = true;
+            }
+            
+            map = new HashMap<Integer, Object>();
+        }
+        
+        public int nextInt() {
+        	return gen.nextInt(map);
+        }
+        
+        public long nextLong() {
+        	return gen.nextInt(map);
+        }
+        
+        public double nextDouble() {
+        	return gen.nextDouble(map);
+        }
+        
+        public float nextFloat() {
+        	return gen.nextFloat(map);
+        }
+        
+        public String nextString() {
+        	return gen.nextString(map);
+        }
+    }
+
+    abstract class RandomGenerator {
+
+        protected int avgsz;
+        protected boolean hasMapFile; // indicating whether a map file from 
+                                      // random number to the field value is pre-defined
+
+        abstract public int nextInt(Map<Integer, Object> map);
+        abstract public long nextLong(Map<Integer, Object> map);
+        abstract public float nextFloat(Map<Integer, Object> map);
+        abstract public double nextDouble(Map<Integer, Object> map);
+        abstract public String nextString(Map<Integer, Object> map);
+
+        public String randomString() {
+            int var = (int)((double)avgsz * 0.3);
+            StringBuffer sb = new StringBuffer(avgsz + var);
+            if (var < 1) var = 1;
+            int len = rand.nextInt(2 * var) + avgsz - var;
+            for (int i = 0; i < len; i++) {
+                int n = rand.nextInt(122 - 65) + 65;
+                sb.append(Character.toChars(n));
+            }
+            return sb.toString();
+        }
+        
+        public float randomFloat() {
+        	return rand.nextFloat() * rand.nextInt();
+        }
+        
+        public double randomDouble() {
+        	return rand.nextDouble() * rand.nextInt();
+        }
+    }
+
+    class UniformRandomGenerator extends RandomGenerator {
+        int card;       
+        
+        public UniformRandomGenerator(int a, int c) {
+            avgsz = a;
+            card = c;            
+        }
+
+        public int nextInt(Map<Integer, Object> map) {
+            return rand.nextInt(card);
+        }
+
+        public long nextLong(Map<Integer, Object> map) {
+            return rand.nextLong() % card;
+        }
+
+        public float nextFloat(Map<Integer, Object> map) {
+            int seed = rand.nextInt(card);
+            Float f = (Float)map.get(seed);
+            if (f == null) {
+            	if (!hasMapFile) {
+            		f = randomFloat();
+            		map.put(seed, f);
+            	}else{
+            		throw new IllegalStateException("Number " + seed + " is not found in map file");
+            	}
+            }
+            return f;
+        }
+        
+        public double nextDouble(Map<Integer, Object> map) {
+            int seed = rand.nextInt(card);
+            Double d = (Double)map.get(seed);
+            if (d == null) {
+            	if (!hasMapFile) {
+            		d = randomDouble();
+            		map.put(seed, d);
+            	}else{
+            		throw new IllegalStateException("Number " + seed + " is not found in map file");
+            	}
+            }
+            return d;
+        }
+
+        public String nextString(Map<Integer, Object> map) {
+            int seed = rand.nextInt(card);
+            String s = (String)map.get(seed);
+            if (s == null) {
+            	if (!hasMapFile) {
+            		s = randomString();
+            		map.put(seed, s);
+            	}else{
+            		throw new IllegalStateException("Number " + seed + " is not found in map file");
+            	}
+            }
+            return s;
+        }
+        
+    }
+
+    class ZipfRandomGenerator extends RandomGenerator {
+        Zipf z;
+
+        public ZipfRandomGenerator(int a, int c) {
+            avgsz = a;
+            z = new Zipf(c);         
+        }
+        
+        
+        // the Zipf library returns a random number [1..cardinality], so we substract by 1
+        // to get [0..cardinality)
+        // the randome number returned by zipf library is an integer, but converted into double
+        private double next() {
+        	return z.nextElement()-1;
+        }
+
+        public int nextInt(Map<Integer, Object> map) {
+            return (int)next();
+        }
+
+        public long nextLong(Map<Integer, Object> map) {
+            return (long)next();
+        }
+
+        public float nextFloat(Map<Integer, Object> map) {
+        	int seed = (int)next();
+            Float d = (Float)map.get(seed);
+            if (d == null) {
+            	if (!hasMapFile) {
+            		d = randomFloat();
+            		map.put(seed, d);
+            	}else{
+            		throw new IllegalStateException("Number " + seed + " is not found in map file");
+            	}
+            }
+            return d;
+        }
+
+        public double nextDouble(Map<Integer, Object> map) {
+        	 int seed = (int)next();
+             Double d = (Double)map.get(seed);
+             if (d == null) {
+             	if (!hasMapFile) {
+             		d = randomDouble();
+             		map.put(seed, d);
+             	}else{
+             		throw new IllegalStateException("Number " + seed + " is not found in map file");
+             	}
+             }
+             return d;
+        }
+        
+        public String nextString(Map<Integer, Object> map) {
+            int seed = (int)next();
+            String s = (String)map.get(seed);
+            if (s == null) {
+            	if (!hasMapFile) {
+            		s = randomString();
+            		map.put(seed, s);
+            	}else{
+            		throw new IllegalStateException("Number " + seed + " is not found in map file");
+            	}
+            }
+            return s;
+        }
+    }
+    
+//  launch hadoop job
+    class HadoopRunner {
+    	Random r;
+    	FileSystem fs;
+    	Path tmpHome;
+    	
+    	public HadoopRunner() {
+    		r = new Random();
+    	}
+    	
+    	public void goHadoop() throws IOException {
+            // Configuration processed by ToolRunner
+            Configuration conf = getConf();
+            
+            // Create a JobConf using the processed conf            
+            JobConf job = new JobConf(conf);     
+            fs = FileSystem.get(job);           
+            
+            tmpHome = createTempDir(null);                          
+            
+            String config = genMapFiles().toUri().getRawPath();
+            // set config properties into job conf
+            job.set("fieldconfig", config);      
+            job.set("separator", String.valueOf((int)separator));
+            
+            
+            job.setJobName("data-gen");
+            job.setNumMapTasks(numMappers);
+            job.setNumReduceTasks(0);          
+            job.setMapperClass(DataGenMapper.class);   
+            job.setJarByClass(DataGenMapper.class);
+            
+            // if inFile is specified, use it as input
+            if (inFile != null) {
+           	 FileInputFormat.setInputPaths(job, inFile);
+           	 job.set("hasinput", "true");
+           } else {
+        	   job.set("hasinput", "false");
+        	   Path input = genInputFiles();        
+        	   FileInputFormat.setInputPaths(job, input);
+           }
+           FileOutputFormat.setOutputPath(job, new Path(outputFile));
+                               
+            // Submit the job, then poll for progress until the job is complete
+            System.out.println("Submit hadoop job...");
+            RunningJob j = JobClient.runJob(job);
+            if (!j.isSuccessful()) {
+            	throw new IOException("Job failed");
+            }
+                        
+            if (fs.exists(tmpHome)) {            	
+            	fs.delete(tmpHome, true);
+            }
+         }
+    	
+    	 private Path genInputFiles() throws IOException {
+    		 long avgRows = numRows/numMappers;
+    	        
+    		 // create a temp directory as mappers input
+    	     Path input = createTempDir(tmpHome);
+    	     System.out.println("Generating input files into " + input.toString());
+    	     
+    	     long rowsLeft = numRows;
+    	     
+    	     // create one input file per mapper, which contains
+    	     // the number of rows 
+    	     for(int i=0; i<numMappers; i++) {
+    	    	 Object[] tmp = createTempFile(input, false);
+    	    	 PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
+    	    	 
+    	    	 if (i < numMappers-1) {
+    	    		 pw.println(avgRows);
+    	    	 }else{
+    	    		 // last mapper takes all the rows left
+    	    		 pw.println(rowsLeft);
+    	    	 }
+    	    
+    	    	 pw.close();
+    	    	 rowsLeft -= avgRows;
+    	     }
+    	     
+    	     return input;
+    	 }
+    	
+    	// generate map files for all the fields that need to pre-generate map files
+    	// return a config file which contains config info for each field, including 
+    	// the path to their map file
+    	 private Path genMapFiles() throws IOException {
+    		 Object[] tmp = createTempFile(tmpHome, false);
+    		 
+    		 System.out.println("Generating column config file in " + tmp[0].toString());
+    		 PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
+    		 for(int i=0; i<colSpecs.length; i++) {
+    			 DataGenerator.Datatype datatype = colSpecs[i].datatype;
+    			 pw.print(colSpecs[i].arg);
+    			 
+    			 if ( datatype == DataGenerator.Datatype.FLOAT || datatype == DataGenerator.Datatype.DOUBLE ||
+    					 datatype == DataGenerator.Datatype.STRING) 	 {
+    				 Path p = genMapFile(colSpecs[i]);
+    				 pw.print(':');    				
+    				 pw.print(p.toUri().getRawPath());				 
+    			 } 
+    			 
+    			 pw.println();
+    		 }
+    		 
+    		 pw.close();
+    		 
+    		 return (Path)tmp[0];
+    	 }
+    	 
+    	 // genereate a map file between random number to field value
+    	 // return the path of the map file
+    	 private Path genMapFile(DataGenerator.ColSpec col) throws IOException {
+    		 int card = col.card;
+    		 Object[] tmp = createTempFile(tmpHome, false);
+    		 
+    		 System.out.println("Generating mapping file for column " + col.arg + " into " + tmp[0].toString());
+    		 PrintWriter pw = new PrintWriter((OutputStream)tmp[1]);
+    		 HashSet<Object> hash = new HashSet<Object>(card);
+    		 for(int i=0; i<card; i++) {
+    			 pw.print(i);
+    			 pw.print("\t");
+    			 Object next = null;
+    			 do {
+    				 if (col.datatype == DataGenerator.Datatype.DOUBLE) {
+        				 next = col.gen.randomDouble();
+        			 }else if (col.datatype == DataGenerator.Datatype.FLOAT) {
+        				 next = col.gen.randomFloat();
+        			 }else if (col.datatype == DataGenerator.Datatype.STRING) {
+        				 next = col.gen.randomString();
+        			 }
+    			 }while(hash.contains(next));
+    			 
+    			 hash.add(next);
+    			 
+    			 pw.println(next);
+    			 
+    			 if ( (i>0 && i%300000 == 0) || i == card-1 ) {
+    				 System.out.println("processed " + i*100/card + "%." );
+    				 pw.flush();
+    			 }    			 
+    		 }    		 
+    		 
+    		 pw.close();
+    		 
+    		 return (Path)tmp[0];
+        	 
+    	 }
+    	 
+    	 private Path createTempDir(Path parentDir) throws IOException {
+    		 Object[] obj = createTempFile(parentDir, true);
+    		 return (Path)obj[0];
+    	 }
+    	 
+    	 private Object[] createTempFile(Path parentDir, boolean isDir) throws IOException {		
+    		 Path tmp_home = parentDir;
+    		 
+    		 if (tmp_home == null) {
+    			 tmp_home = new Path(fs.getHomeDirectory(), "tmp");
+    		 }
+    		 
+    		 if (!fs.exists(tmp_home)) {
+    			 fs.mkdirs(tmp_home);
+    		 }
+    		 
+    		 int id = r.nextInt();
+    		 Path f = new Path(tmp_home, "tmp" + id);
+    		 while (fs.exists(f)) {
+    			 id = r.nextInt();
+    			 f = new Path(tmp_home, "tmp" + id);
+    		 }
+    		 
+    		 // return a 2-element array. first element is PATH,
+    		 // second element is OutputStream
+    		 Object[] r = new Object[2];
+    		 r[0] = f;
+    		 if (!isDir) {
+    			 r[1] = fs.create(f);
+    		 }else{
+    			 fs.mkdirs(f);
+    		 }
+    		 
+    		 return r;
+    	 }    	
+    }
+
+	 public static class DataGenMapper extends MapReduceBase implements Mapper<LongWritable, Text, String, String> {
+ 		private JobConf jobConf;
+ 		private DataGenerator dg; 		
+ 		private boolean hasInput;
+ 	   
+ 		public void configure(JobConf jobconf) {
+ 			this.jobConf = jobconf;
+ 						 			
+ 			int id = Integer.parseInt(jobconf.get("mapred.task.partition"));
+ 			long time = System.currentTimeMillis() - id*3600*24*1000;
+ 			 			
+ 			dg = new DataGenerator( ((time-id*3600*24*1000) | (id << 48)));
+ 			
+ 		    dg.separator = (char)Integer.parseInt(jobConf.get("separator"));
+ 		     		    
+ 		    if (jobConf.get("hasinput").equals("true")) {
+ 		    	hasInput = true;
+ 		    }
+ 		    
+ 		    String config = jobConf.get("fieldconfig");
+ 		    		    
+ 		    try {			    
+ 			    FileSystem fs = FileSystem.get(jobconf);
+ 			    
+ 			    // load in config file for each column
+ 			    BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(new Path(config))));
+ 			    String line = null;
+ 			    List<DataGenerator.ColSpec> cols = new ArrayList<DataGenerator.ColSpec>();
+ 			    while((line = reader.readLine()) != null) { 			    	
+ 			    	cols.add(dg.new ColSpec(line));		    	
+ 			    }
+ 			    reader.close();
+ 			    dg.colSpecs = cols.toArray(new DataGenerator.ColSpec[0]); 			    
+ 			    
+ 			    // load in mapping files
+ 			    for(int i=0; i<dg.colSpecs.length; i++) {
+ 			    	DataGenerator.ColSpec col = dg.colSpecs[i];
+ 			    	if (col.mapfile != null) { 			    		
+ 			    		reader = new BufferedReader(new InputStreamReader(fs.open(new Path(col.mapfile))));
+ 			    		Map<Integer, Object> map = dg.colSpecs[i].map;
+ 			    		while((line = reader.readLine()) != null) {
+ 					    	String[] fields = line.split("\t");
+ 					    	int key = Integer.parseInt(fields[0]);
+ 					    	if (col.datatype == DataGenerator.Datatype.DOUBLE) {
+ 					    		map.put(key, Double.parseDouble(fields[1]));
+ 					    	}else if (col.datatype == DataGenerator.Datatype.FLOAT) {
+ 					    		map.put(key, Float.parseFloat(fields[1]));
+ 					    	}else {
+ 					    		map.put(key, fields[1]);
+ 					    	}
+ 					    }
+ 			    		
+ 			    		reader.close();
+ 			    	}
+ 			    }
+ 		    }catch(IOException e) {
+ 		    	throw new RuntimeException("Failed to load config file. " + e);
+ 		    }
+  		}
+ 		
+ 		public void map(LongWritable key, Text value, OutputCollector<String, String> output, Reporter reporter) throws IOException {
+ 			int intialsz = dg.colSpecs.length * 50;
+ 			
+ 			if (!hasInput) {
+ 				long numRows = Long.parseLong(value.toString().trim()); 		
+ 	 	        dg.numRows = numRows; 	 	         	       
+ 	 	        
+ 	 	        for (int i = 0; i < numRows; i++) {
+ 	 	        	StringWriter str = new StringWriter(intialsz);
+ 	 	        	PrintWriter pw = new PrintWriter(str);
+ 	 	        	dg.writeLine(pw); 	    
+ 	 	        	output.collect(null, str.toString()); 	        	 	        
+ 	 	        	
+ 	 	        	if ((i+1) % 10000 == 0) {
+ 	 	        		reporter.progress();
+ 	 	        		reporter.setStatus("" + (i+1) + " of " + numRows + " tuples generated.");  	        	
+ 	 	        	}
+ 	 	        }	       
+ 			} else {
+ 				StringWriter str = new StringWriter(intialsz);
+	 	        PrintWriter pw = new PrintWriter(str);
+	 	        pw.write(value.toString());
+	 	        dg.writeLine(pw); 	    
+	 	        output.collect(null, str.toString()); 	        	 	        	 	        	
+ 			}
+ 		}
+ 	}
+}
+
+
+
+
diff --git test/utils/pigmix/scripts/L1.pig test/utils/pigmix/scripts/L1.pig
new file mode 100644
index 0000000..dca811e
--- /dev/null
+++ test/utils/pigmix/scripts/L1.pig
@@ -0,0 +1,15 @@
+-- This script tests reading from a map, flattening a bag of maps, and use of bincond.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, (int)action as action, (map[])page_info as page_info,
+    flatten((bag{tuple(map[])})page_links) as page_links;
+C = foreach B generate user,
+    (action == 1 ? page_info#'a' : page_links#'b') as header;
+D = group C by user;
+E = foreach D generate group, COUNT(C) as cnt;
+store E into 'L1out';
+
+
diff --git test/utils/pigmix/scripts/L10.pig test/utils/pigmix/scripts/L10.pig
new file mode 100644
index 0000000..1b4a2d5
--- /dev/null
+++ test/utils/pigmix/scripts/L10.pig
@@ -0,0 +1,8 @@
+--This script covers order by of multiple values.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent:int, query_term, ip_addr, timestamp,
+        estimated_revenue:double, page_info, page_links);
+B = order A by query_term, estimated_revenue desc, timespent;
+store B into 'L10out';
diff --git test/utils/pigmix/scripts/L11.pig test/utils/pigmix/scripts/L11.pig
new file mode 100644
index 0000000..5e9a424
--- /dev/null
+++ test/utils/pigmix/scripts/L11.pig
@@ -0,0 +1,14 @@
+-- This script covers distinct and union.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user;
+C = distinct B;
+alpha = load '$PIGMIX_DIR/widerow' using PigStorage('\u0001');
+beta = foreach alpha generate $0 as name;
+gamma = distinct beta;
+D = union C, gamma;
+E = distinct D;
+store E into 'L11out';
diff --git test/utils/pigmix/scripts/L12.pig test/utils/pigmix/scripts/L12.pig
new file mode 100644
index 0000000..f54bd57
--- /dev/null
+++ test/utils/pigmix/scripts/L12.pig
@@ -0,0 +1,19 @@
+-- This script covers multi-store queries.
+register pigperf.jar;
+%default PIGMIX_DIR $PIGMIX_DIR
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, action, (int)timespent as timespent, query_term,
+    (double)estimated_revenue as estimated_revenue;
+split B into C if user is not null, alpha if user is null;
+split C into D if query_term is not null, aleph if query_term is null;
+E = group D by user;
+F = foreach E generate group, MAX(D.estimated_revenue);
+store F into 'highest_value_page_per_user';
+beta = group alpha by query_term;
+gamma = foreach beta generate group, SUM(alpha.timespent);
+store gamma into 'total_timespent_per_term';
+beth = group aleph by action;
+gimel = foreach beth generate group, COUNT(aleph);
+store gimel into 'queries_per_action';
diff --git test/utils/pigmix/scripts/L13.pig test/utils/pigmix/scripts/L13.pig
new file mode 100644
index 0000000..c21a4d3
--- /dev/null
+++ test/utils/pigmix/scripts/L13.pig
@@ -0,0 +1,10 @@
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+	as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = foreach A generate user, estimated_revenue;
+alpha = load '$PIGMIX_DIR/power_users_samples' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
+beta = foreach alpha generate name, phone;
+C = join B by user left outer, beta by name;
+store C into 'L13out';
+
diff --git test/utils/pigmix/scripts/L14.pig test/utils/pigmix/scripts/L14.pig
new file mode 100644
index 0000000..a187228
--- /dev/null
+++ test/utils/pigmix/scripts/L14.pig
@@ -0,0 +1,10 @@
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views_sorted' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = foreach A generate user, estimated_revenue;
+alpha = load '$PIGMIX_DIR/users_sorted' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
+beta = foreach alpha generate name;
+C = join B by user, beta by name using "merge";
+store C into 'L14out';
+
diff --git test/utils/pigmix/scripts/L15.pig test/utils/pigmix/scripts/L15.pig
new file mode 100644
index 0000000..e4d226e
--- /dev/null
+++ test/utils/pigmix/scripts/L15.pig
@@ -0,0 +1,14 @@
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = foreach A generate user, action, estimated_revenue, timespent;
+C = group B by user;
+D = foreach C {
+    beth = distinct B.action;
+    rev = distinct B.estimated_revenue;
+    ts = distinct B.timespent;
+    generate group, COUNT(beth), SUM(rev), (int)AVG(ts);
+}
+store D into 'L15out';
+
diff --git test/utils/pigmix/scripts/L16.pig test/utils/pigmix/scripts/L16.pig
new file mode 100644
index 0000000..84ab1f7
--- /dev/null
+++ test/utils/pigmix/scripts/L16.pig
@@ -0,0 +1,14 @@
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = foreach A generate user, estimated_revenue;
+C = group B by user;
+D = foreach C {
+    E = order B by estimated_revenue;
+    F = E.estimated_revenue;
+    generate group, SUM(F);
+}
+
+store D into 'L16out';
+
diff --git test/utils/pigmix/scripts/L17.pig test/utils/pigmix/scripts/L17.pig
new file mode 100644
index 0000000..7da8fb4
--- /dev/null
+++ test/utils/pigmix/scripts/L17.pig
@@ -0,0 +1,14 @@
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIRwidegroupbydata' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links, user_1, action_1, timespent_1, query_term_1, ip_addr_1, timestamp_1,
+        estimated_revenue_1, page_info_1, page_links_1, user_2, action_2, timespent_2, query_term_2, ip_addr_2, timestamp_2,
+        estimated_revenue_2, page_info_2, page_links_2);
+B = group A by (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, user_1, action_1, timespent_1, query_term_1, ip_addr_1, timestamp_1,
+        estimated_revenue_1, user_2, action_2, timespent_2, query_term_2, ip_addr_2, timestamp_2,
+        estimated_revenue_2);
+C = foreach B generate SUM(A.timespent), SUM(A.timespent_1), SUM(A.timespent_2), AVG(A.estimated_revenue), AVG(A.estimated_revenue_1), AVG(A.estimated_revenue_2);
+store C into 'L17out';
+
diff --git test/utils/pigmix/scripts/L2.pig test/utils/pigmix/scripts/L2.pig
new file mode 100644
index 0000000..35968eb
--- /dev/null
+++ test/utils/pigmix/scripts/L2.pig
@@ -0,0 +1,13 @@
+-- This script tests using a join small enough to do in fragment and replicate. 
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, estimated_revenue;
+alpha = load '$PIGMIX_DIR/power_users' using PigStorage('\u0001') as (name, phone,
+        address, city, state, zip);
+beta = foreach alpha generate name;
+C = join B by user, beta by name using 'replicated';
+store C into 'L2out';
+
diff --git test/utils/pigmix/scripts/L3.pig test/utils/pigmix/scripts/L3.pig
new file mode 100644
index 0000000..c09f4e3
--- /dev/null
+++ test/utils/pigmix/scripts/L3.pig
@@ -0,0 +1,17 @@
+--This script tests a join too large for fragment and replicate.  It also 
+--contains a join followed by a group by on the same key, something that we
+--could potentially optimize by not regrouping.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, (double)estimated_revenue;
+alpha = load '$PIGMIX_DIR/users' using PigStorage('\u0001') as (name, phone, address,
+        city, state, zip);
+beta = foreach alpha generate name;
+C = join beta by name, B by user;
+D = group C by $0;
+E = foreach D generate group, SUM(C.estimated_revenue);
+store E into 'L3out';
+
diff --git test/utils/pigmix/scripts/L4.pig test/utils/pigmix/scripts/L4.pig
new file mode 100644
index 0000000..ac006d7
--- /dev/null
+++ test/utils/pigmix/scripts/L4.pig
@@ -0,0 +1,14 @@
+-- This script covers foreach/generate with a nested distinct.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, action;
+C = group B by user;
+D = foreach C {
+    aleph = B.action;
+    beth = distinct aleph;
+    generate group, COUNT(beth);
+}
+store D into 'L4out';
diff --git test/utils/pigmix/scripts/L5.pig test/utils/pigmix/scripts/L5.pig
new file mode 100644
index 0000000..bf6a2ea
--- /dev/null
+++ test/utils/pigmix/scripts/L5.pig
@@ -0,0 +1,15 @@
+--This script does an anti-join.  This is useful because it is a use of
+--cogroup that is not a regular join.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user;
+alpha = load '$PIGMIX_DIR/users' using PigStorage('\u0001') as (name, phone, address,
+        city, state, zip);
+beta = foreach alpha generate name;
+C = cogroup beta by name, B by user;
+D = filter C by COUNT(beta) == 0;
+E = foreach D generate group;
+store E into 'L5out';
diff --git test/utils/pigmix/scripts/L6.pig test/utils/pigmix/scripts/L6.pig
new file mode 100644
index 0000000..adbf993
--- /dev/null
+++ test/utils/pigmix/scripts/L6.pig
@@ -0,0 +1,12 @@
+-- This script covers the case where the group by key is a significant
+-- percentage of the row.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, action, (int)timespent as timespent, query_term, ip_addr, timestamp;
+C = group B by (user, query_term, ip_addr, timestamp);
+D = foreach C generate flatten(group), SUM(B.timespent);
+store D into 'L6out';
+
diff --git test/utils/pigmix/scripts/L7.pig test/utils/pigmix/scripts/L7.pig
new file mode 100644
index 0000000..6db181c
--- /dev/null
+++ test/utils/pigmix/scripts/L7.pig
@@ -0,0 +1,13 @@
+-- This script covers having a nested plan with splits.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader() as (user, action, timespent, query_term,
+            ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = foreach A generate user, timestamp;
+C = group B by user;
+D = foreach C {
+    morning = filter B by timestamp < 43200;
+    afternoon = filter B by timestamp >= 43200;
+    generate group, COUNT(morning), COUNT(afternoon);
+}
+store D into 'L7out';
diff --git test/utils/pigmix/scripts/L8.pig test/utils/pigmix/scripts/L8.pig
new file mode 100644
index 0000000..c9c02f5
--- /dev/null
+++ test/utils/pigmix/scripts/L8.pig
@@ -0,0 +1,10 @@
+-- This script covers group all.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, (int)timespent as timespent, (double)estimated_revenue as estimated_revenue;
+C = group B all;
+D = foreach C generate SUM(B.timespent), AVG(B.estimated_revenue);
+store D into 'L8out';
diff --git test/utils/pigmix/scripts/L9.pig test/utils/pigmix/scripts/L9.pig
new file mode 100644
index 0000000..3b95b2d
--- /dev/null
+++ test/utils/pigmix/scripts/L9.pig
@@ -0,0 +1,8 @@
+--This script covers order by of a single value.
+register pigperf.jar;
+%default PIGMIX_DIR /user/pig/tests/data/pigmix
+A = load '$PIGMIX_DIR/page_views' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = order A by query_term;
+store B into 'L9out';
diff --git test/utils/pigmix/scripts/generate_data.sh test/utils/pigmix/scripts/generate_data.sh
new file mode 100644
index 0000000..82afe1b
--- /dev/null
+++ test/utils/pigmix/scripts/generate_data.sh
@@ -0,0 +1,233 @@
+#!/bin/sh
+
+java=${JAVA_HOME:='/usr'}/bin/java;
+#hadoop_ops="-D mapred.job.queue.name=grideng -D mapreduce.job.acl-view-job=*"
+
+if [ ! -e $java ] 
+then
+    echo "Cannot find java, set JAVA_HOME environment variable to directory above bin/java."
+    exit
+fi
+
+PIG_HOME=${PIG_HOME:='.'} 
+pigjar=$PIG_HOME/pig.jar
+testjar=pigperf.jar
+if [ ! -e $pigjar ]
+then
+    echo "Cannot find pig.jar, set PIG_HOME environment variable to directory pig.jar and build directory are in"
+    exit
+fi
+
+if [ -z "$PIGMIX_DIR" ]
+then 
+    PIGMIX_DIR=/user/pig/tests/data/pigmix
+fi
+	
+zipfjar=$PIG_HOME/lib/sdsuLibJKD12.jar
+
+classpath=$pigjar:$zipfjar:$testjar
+
+export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$classpath
+
+# configure the cluster
+conf_dir=$HADOOP_CONF_DIR
+
+# configure the number of mappers, if 0, job is run locally
+mappers=90
+
+mainclass=org.apache.pig.test.utils.datagen.DataGenerator 
+
+rows=625000000
+
+
+user_field=s:20:1600000:z:7
+action_field=i:1:2:u:0
+os_field=i:1:20:z:0
+query_term_field=s:10:1800000:z:20
+ip_addr_field=l:1:1000000:z:0
+timestamp_field=l:1:86400:z:0
+estimated_revenue_field=d:1:100000:z:5
+page_info_field=m:10:1:z:0
+page_links_field=bm:10:1:z:20
+
+pages=pages625m
+
+echo "Generating $pages"
+
+$HADOOP_HOME/bin/hadoop --config $conf_dir jar pigperf.jar $mainclass $hadoop_ops \
+    -m $mappers -r $rows -f $pages $user_field \
+    $action_field $os_field $query_term_field $ip_addr_field \
+    $timestamp_field $estimated_revenue_field $page_info_field \
+    $page_links_field
+
+
+# Skim off 1 in 10 records for the user table
+# Be careful the file is in HDFS if you run previous job as hadoop job, 
+# you should either copy data into local disk to run following script
+# or run hadoop job to trim the data
+
+protousers=users
+echo "Skimming users"
+java -Xmx1024m -cp $HADOOP_CLASSPATH:$conf_dir:$PIG_HOME/pig.jar org.apache.pig.Main << EOF
+register pigperf.jar;
+fs -rmr users;
+A = load '$pages' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = foreach A generate user;
+C = distinct B parallel $mappers;
+D = order C by \$0 parallel $mappers;
+store D into '$protousers';
+EOF
+
+
+# Create users table, with now user field.
+phone_field=s:10:1600000:z:20
+address_field=s:20:1600000:z:20
+city_field=s:10:1600000:z:20
+state_field=s:2:1600:z:20
+zip_field=i:2:1600:z:20
+
+users=users100m
+
+echo "Generating $users"
+
+$HADOOP_HOME/bin/hadoop --config $conf_dir jar pigperf.jar $mainclass $hadoop_ops \
+    -m $mappers -i $protousers -f $users $phone_field \
+    $address_field $city_field $state_field $zip_field
+
+# Find unique keys for fragment replicate join testing
+# If file is in HDFS, extra steps are required
+numuniquekeys=500
+protopowerusers=power_users
+echo "Skimming power users"
+
+java -Xmx1024m -cp $HADOOP_CLASSPATH:$conf_dir:$PIG_HOME/pig.jar org.apache.pig.Main << EOF
+register pigperf.jar;
+fs -rmr $protopowerusers;
+A = load '$pages' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = foreach A generate user;
+C = distinct B parallel $mappers;
+D = order C by \$0 parallel $mappers;
+E = limit D 500;
+store E into '$protopowerusers';
+EOF
+
+echo "Generating $powerusers"
+
+powerusers=power_users100m
+
+$HADOOP_HOME/bin/hadoop --config $conf_dir jar pigperf.jar $mainclass $hadoop_ops \
+    -m $mappers -i $protopowerusers -f $powerusers $phone_field \
+    $address_field $city_field $state_field $zip_field
+
+echo "Generating widerow"
+
+widerowcnt=10000000
+widerows=widerow10m
+user_field=s:20:10000:z:0
+int_field=i:1:10000:u:0 
+
+$HADOOP_HOME/bin/hadoop --config $conf_dir jar pigperf.jar $mainclass $hadoop_ops \
+    -m $mappers -r $widerowcnt -f $widerows $user_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field \
+    $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field $int_field
+
+java -Xmx1024m -cp $HADOOP_CLASSPATH:$conf_dir:$PIG_HOME/pig.jar org.apache.pig.Main << EOF
+register pigperf.jar;
+fs -rmr ${pages}_sorted;
+fs -rmr ${users}_sorted;
+fs -rmr ${powerusers}_samples;
+A = load '$pages' using org.apache.pig.test.udf.storefunc.PigPerformanceLoader()
+    as (user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links);
+B = order A by user parallel $mappers;
+store B into '${pages}_sorted' using PigStorage('\u0001');
+
+alpha = load '$users' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
+a1 = order alpha by name parallel $mappers;
+store a1 into '${users}_sorted' using PigStorage('\u0001');
+
+a = load '$powerusers' using PigStorage('\u0001') as (name, phone, address, city, state, zip);
+b = sample a 0.5;
+store b into '${powerusers}_samples' using PigStorage('\u0001');
+
+A = load '$pages' as (user, action, timespent, query_term, ip_addr, timestamp,
+        estimated_revenue, page_info, page_links);
+B = foreach A generate user, action, timespent, query_term, ip_addr, timestamp, estimated_revenue, page_info, page_links,
+user as user1, action as action1, timespent as timespent1, query_term as query_term1, ip_addr as ip_addr1, timestamp as timestamp1, estimated_revenue as estimated_revenue1, page_info as page_info1, page_links as page_links1,
+user as user2, action as action2, timespent as timespent2, query_term as query_term2, ip_addr as ip_addr2, timestamp as timestamp2, estimated_revenue as estimated_revenue2, page_info as page_info2, page_links as page_links2;
+store B into 'widegroupbydata';
+EOF
+
+mkdir $powerusers
+java -Xmx1024m -cp $HADOOP_CLASSPATH:$conf_dir:$PIG_HOME/pig.jar org.apache.pig.Main << EOF
+fs -copyToLocal ${powerusers}/* $powerusers;
+EOF
+
+cat $powerusers/* > power_users
+
+java -Xmx1024m -cp $HADOOP_CLASSPATH:$conf_dir:$PIG_HOME/pig.jar org.apache.pig.Main << EOF
+fs -rmr $protousers;
+fs -rmr $protopowerusers;
+fs -rmr $powerusers;
+fs -mkdir $PIGMIX_DIR;
+fs -mv $pages $PIGMIX_DIR/page_views;
+fs -mv ${pages}_sorted $PIGMIX_DIR/page_views_sorted;
+fs -mv $users $PIGMIX_DIR/users;
+fs -mv ${users}_sorted $PIGMIX_DIR/users_sorted;
+fs -copyFromLocal power_users $PIGMIX_DIR/power_users;
+fs -mv ${powerusers}_samples $PIGMIX_DIR/power_users_samples;
+fs -mv widegroupbydata $PIGMIX_DIR/widegroupbydata;
+fs -mv $widerows $PIGMIX_DIR/widerow;
+EOF
+
+rm -fr $powerusers
+rm power_users
diff --git test/utils/pigmix/scripts/runpigmix-adhoc.pl test/utils/pigmix/scripts/runpigmix-adhoc.pl
new file mode 100644
index 0000000..8a688d1
--- /dev/null
+++ test/utils/pigmix/scripts/runpigmix-adhoc.pl
@@ -0,0 +1,109 @@
+#!/usr/bin/env perl 
+
+use strict; 
+use warnings;
+
+if(scalar(@ARGV) < 3 )
+{
+    print STDERR "Usage: $0 <pigjar> <pigperfjar> <dir containing hadoopsitexml> <pig mix scripts dir> [numruns] [runmapreduce] \n";
+    exit(-1);
+}
+my $pigjar=shift;
+my $pigperfjar=shift;
+my $confdir=shift;
+my $scriptdir = shift;
+my $runs = shift;
+my $runmapreduce = shift;
+if(!defined($runs)) {
+    $runs = 3;
+}
+if(!defined($runmapreduce)) {
+    $runmapreduce = 1;
+}
+
+my $java;
+my $java_home = $ENV{'JAVA_HOME'};
+my $classpath= $ENV{'HADOOP_CLASSPATH'};
+if(!defined($java_home)) {
+    $java = "/usr/bin/java";
+} else {
+    $java = $java_home."/bin/java";
+}
+
+my $pigmix_dir = $ENV{'PIGMIX_DIR'} || "/user/pig/tests/data/pigmix";
+my $hadoop_opts = "-DPIGMIX_DIR=$pigmix_dir"; #"-Dmapred.job.queue.name=grideng -Dmapreduce.job.acl-view-job=*";
+
+
+
+my $cmd;
+my $total_pig_times = 0;
+my $total_mr_times = 0;
+for(my $i = 1; $i <= 17; $i++) {
+    my $pig_times = 0;
+    for(my $j = 0; $j < $runs; $j++) {
+        $cmd = "$java $hadoop_opts -cp $classpath:$pigjar:$confdir org.apache.pig.Main -param PIGMIX_DIR=$pigmix_dir $scriptdir/L". $i.".pig" ;
+        print STDERR "Running Pig Query L".$i."\n";
+        print STDERR "L".$i.":";
+        print STDERR "Going to run $cmd\n";
+        my $s = time();
+        print STDERR `$cmd 2>&1`;
+        my $e = time();
+        $pig_times += $e - $s;
+        cleanup($i);
+    }
+    # find avg
+    $pig_times = $pig_times/$runs;
+    # round to next second
+    $pig_times = int($pig_times + 0.5);
+    $total_pig_times = $total_pig_times + $pig_times;
+
+    if ($runmapreduce==0) {
+        print "PigMix_$i pig run time: $pig_times\n";
+    }
+    else {
+        my $mr_times = 0;
+        for(my $j = 0; $j < $runs; $j++) {
+            $cmd = "$java $hadoop_opts -cp $classpath:$pigjar:$confdir:pigperf.jar org.apache.pig.test.pigmix.mapreduce.L$i\n";
+            print STDERR "Running Map-Reduce Query L".$i."\n";
+            print STDERR "L".$i.":";
+            print STDERR "Going to run $cmd";
+            my $s = time();
+            print STDERR `$cmd 2>&1`;
+            my $e = time();
+            $mr_times += $e - $s;
+            cleanup($i);
+        }
+        # find avg
+        $mr_times = $mr_times/$runs;
+        # round to next second
+        $mr_times = int($mr_times + 0.5);
+        $total_mr_times = $total_mr_times + $mr_times;
+
+        my $multiplier = $pig_times/$mr_times;
+        print "PigMix_$i pig run time: $pig_times, java run time: $mr_times, multiplier: $multiplier\n";
+    }
+}
+
+if ($runmapreduce==0) {
+    print "Total pig run time: $total_pig_times\n";
+}
+else {
+    my $total_multiplier = $total_pig_times / $total_mr_times;
+    print "Total pig run time: $total_pig_times, total java time: $total_mr_times, multiplier: $total_multiplier\n";
+}
+
+sub cleanup {
+    my $suffix = shift;
+    my $cmd;
+    $cmd = "$java -cp $classpath:$pigjar:$confdir org.apache.pig.Main -e rmf L".$suffix."out";
+    print STDERR `$cmd 2>&1`;
+    $cmd = "$java -cp $classpath:$pigjar:$confdir org.apache.pig.Main -e rmf highest_value_page_per_user";
+    print STDERR `$cmd 2>&1`;
+    $cmd = "$java -cp $classpath:$pigjar:$confdir org.apache.pig.Main -e rmf total_timespent_per_term";
+    print STDERR `$cmd 2>&1`;
+    $cmd = "$java -cp $classpath:$pigjar:$confdir org.apache.pig.Main -e rmf queries_per_action";
+    print STDERR `$cmd 2>&1`;
+    $cmd = "$java -cp $classpath:$pigjar:$confdir org.apache.pig.Main -e rmf tmp";
+    print STDERR `$cmd 2>&1`;
+}
+
